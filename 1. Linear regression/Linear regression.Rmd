---
t = t---
title: "1. Linear regression"
author: "Robert Lohne"
date: "13 6 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

############################################################
## Define packages to be loaded, and if needed, installed ##
##                                                        ##    
## by robert lohne (rlohne@gmail.com / @robertlohne)      ##
############################################################

# 1 Definitions
# 1.1 Package names 
# Define the names of the packages to be used, these are stored in the packages variable, and loaded/installed as needed
packages <- c("gapminder", "dplyr", "ggplot2", "rsample", "gridExtra", "tidymodels", "Metrics")

# 1.1 Packages description
# 

# 2. Install packages not yet installed
# Installed packages are stored in the installed packages variable
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# 3. Packages loading
invisible(lapply(packages, library, character.only = TRUE))

```

# 1. What *is* Linear regression?

Linear regression is a simple method for statistical learning. In it's simplest form (aptly named simple regression), it deals with a single explanatory variable (sometimes called the predictor), denoted $X$ and the dependent variable (sometimes referred to as the outcome variable), denoted $Y$. The assumption is that there is a relationship between the two, and that $X$ can be used to predict the value of $Y$. This is usually expressed as $Y = f(X) + \epsilon$ .\
\
$f$ is in this case a function of $X$, and $\epsilon$ (greek letter epsilon) is a random error term, that is independent of $X$ and has a mean zero. As an example, consider the following assumption: the miles per gallon (mpg) feature of a car has a relationship with it's weight, and we assume that the lower the weight, the higher the mpg.

```{r ggplot example_negative_correlation_lm, echo=FALSE, }
df <- mtcars
model <- lm(mpg ~ wt, data = df)
df$predicted <- predict(model)
df$residuals <- residuals(model)


ggplot(df, 
       aes(x = wt,
           y = mpg)) +
  geom_point() + 
  stat_smooth(se = FALSE, 
              method = "lm",
              formula = y ~ x) +
  labs(
    title = "Cars with low weight tend to get more mpg than heavier cars",
    caption = "Source: 1974 Motor Trend US"
  )
```

The figure above shows the relationship. The points are the datapoints with x and y values for each car in the dataset, and the blue line is the regression line created by the formula $Y = f(X) + \epsilon$ . Going from a visual inspection alone, this seems to be a reasonable assumption. The points are fairly close to the regression line, and as we increase the value of $X$, the value of $Y$ decreases. This is called a negative linear relationship.

The relationship can also be positive; in that case, increasing the value of $X$ also increases the value of $Y$. Using the example from above, one might hypothesize that the amount of horsepower a car has, depends on the displacement of the car (displacement being a measure of the cylinder volume swept by the pistons).

```{r ggplot example_positive_correlation_lm, echo=FALSE, }
df <- mtcars
model <- lm(hp ~ disp, data = df)
df$predicted <- predict(model)
df$residuals <- residuals(model)


ggplot(df, 
       aes(x = disp,
           y = hp)) +
  geom_point() + 
  stat_smooth(se = FALSE, 
              method = "lm",
              formula = y ~ x) +
  labs(
    title = "Cars with higher displacement tend to have more horsepower",
    caption = "Source: 1974 Motor Trend US"
  )

```

And indeed, this seems to be the case. Compare the figure above with the previous though. Upon visual inspection, there seems to be a clear linear relationship, but as we move towards the higher value of disp, there is a notably larger distance between the regression line and the values of $Y$. This begs the question; how do we assess our models?

## 1.1 How accurate is my model?

In a traditional regression analysis the variance in $Y$ explained by $X$ is denoted $R^2$. This is scored between 0 and 1. 0, or a $R^2$ score of 0.00 means 0 % of the variance is explained by the model, and 1 (100 %) means that all the variance is explained by the model. OK - so what is a "good" value of $R^2$? Well, this is where it gets complicated. It depends on the field, the context and the variables among other things, and is out of scope for a short description that is growing longer by the minute.

Let's revisit the two previous models; mpg explained by weight, and horsepower explained by displacement.

In R, a simple way of defining a linear model is to use the lm() function, creating a model object.

```{r example_lm}

linear_model <- lm(mpg ~ wt, data = mtcars)
```

The syntax is lm(), and the most common arguments are *formula* and *data*. For further arguments, consult the documentation available.

The formula is given in a specific syntax, where the outcome variable (denoted $Y$) is defined on the left-hand side of the \~ (tilde) symbol, and the explanatory variable(s) on the right-hand side (denoted $X$). In our first example, the formula would be given as: mpg \~ wt. This can be read as "mpg explained by weight".

The call to lm() creates a model object, and the results can be seen using various methods. The standard one being summary().

```{r example_lm}

linear_model <- lm(mpg ~ wt, data = mtcars)
summary(linear_model)
```

### 1.1.1$R^2$

The summary lists various summaries of the model object that we might be interested in, such as the residuals, and the coefficients of the model. Additionally, we are given the $R^2$ score, as well as another important figure, the P-value (whether or not the relationship is statistically significant). Another option is to use the broom package. Using either tidy() or glance() gives us many of the same results, but in various other formats. When dealing with a single predictor, $R^2$ is fine, but as you add more predictors, you tend to get small increases in $R^2$, although very small unless there is actually a relationship. The Adjusted $R^2$ adjusts for the number of predictors, and should thus be used if there are more than one predictor. In our mtcars example explaining mpg by weight, we have only one, so let's look at the $R^2$. 0,75 - or 75 % of the variance in $Y$ (mpg) can be explained our model. Ok, so not bad. Let's look at our second mtcars example. Going from a visual inspection of the scatterplot, we hypothesized that this might have a weaker correlation. Let's see:\

```{r example_lm_2}

linear_model <- lm(hp ~ disp, data = mtcars)
summary(linear_model)
```

And indeed it seems that we were right! $R^2$ for the second model is 0,625, or 63 %. You can also see from the P-values of the summaries that both models are significant. While this does not imply causality, there is a very small chance that the variance explained by the model is just random chance. If our null hypothesis were that there are no relationship between mpg and weight or between horsepower and displacement, we could reject those.

### 1.1.2 Residual standard error

Another useful metric to look at is also available in the summary() output; the residual standard error or RSE. Recall the regression formula: $Y = f(X) + \epsilon$ where $\epsilon$ is the term for random errors. This is what is expressed in RSE; an estimate of the standard deviation of $\epsilon$. Roughly, this means that the RSE score is the average the output of the model will deviate from the true regression line.

Looking at our first model; the RSE is scored at 3.046. The mean value of all mpg-values is 20.09, which means that the average deviation is around 15 % off. In our second model, the RSE score is 42.65. Note that the RSE score is given in the same units as the outcome variable, in this case horsepower (hp). The mean hp-value in the mtcars dataset is 146.68, which means that the RSE is quite large; and on average the deviation will be around 29 % off. This ties in with the $R^2$ score as well; the first model explained more of the variation in the outcome variable that the second.

### 1.1.3 **Root Mean Square Error** (RMSE)

The RMSE is another similar metric for evaluating model performance. It measures the standard deviation of the residuals, which also tells us how far off the regression line the predictions are. RMSE can either be calculated by hand, or calculated by the yardstick package, where you can specify which metrics you want. Let's see an example where we get two of the three we've taken a look at:

```{r metrics_example}
df <- mtcars
model <- lm(mpg ~ wt, data = df)
df$predicted <- predict(model)
mtcars_metrics <- metric_set(rsq, rmse)
mtcars_metrics(df, truth = mpg, estimate = predicted)
```

### 1.1.4 Residuals

Residuals are another thing to consider.

```{r ggplot example_residuals, echo=FALSE, }
df <- mtcars
model <- lm(mpg ~ wt, data = df)
df$predicted <- predict(model)
df$residuals <- residuals(model)



ggplot(df, aes(x = wt, y = mpg)) +
  geom_smooth(method = "lm", se = FALSE, color = "grey") +
  geom_segment(aes(xend = wt, yend = predicted), alpha = 0.4, color = "red") +
  geom_point(aes(color = residuals)) +  
  scale_color_gradient2(low = "lightblue", mid = "orange", high = "red") +  
  guides(color = FALSE) +
  geom_point(aes(y = predicted), shape = 1) + 
  labs(title = "Cars with lower weigth tend te get better mileage",
       caption = "Source: 1974 Motor Trend US") + 
  theme_bw()
```
