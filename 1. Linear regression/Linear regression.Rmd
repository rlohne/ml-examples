---
title: "1. Linear regression"
author: "Robert Lohne"
date: "13 6 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

############################################################
## Define packages to be loaded, and if needed, installed ##
##                                                        ##    
## by robert lohne (rlohne@gmail.com / @robertlohne)      ##
############################################################

# 1 Definitions
# 1.1 Package names 
# Define the names of the packages to be used, these are stored in the packages variable, and loaded/installed as needed
packages <- c("gapminder", "dplyr", "ggplot2", "rsample", "gridExtra", "tidymodels")

# 1.1 Packages description
# 

# 2. Install packages not yet installed
# Installed packages are stored in the installed packages variable
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# 3. Packages loading
invisible(lapply(packages, library, character.only = TRUE))

```

# 1. What *is* Linear regression?

Linear regression is a simple method for statistical learning. In it's simplest form (aptly named simple regression), it deals with a single explanatory variable (sometimes called the predictor), denoted $X$ and the dependent variable (sometimes referred to as the outcome variable), denoted $Y$. The assumption is that there is a relationship between the two, and that $X$ can be used to predict the value of $Y$. This is usually expressed as $Y = f(X) + \epsilon$ .\
\
$f$ is in this case a function of $X$, and $\epsilon$ (greek letter epsilon) is a random error term, that is independent of $X$ and has a mean zero. As an example, consider the following assumption: the miles per gallon (mpg) feature of a car has a relationship with it's weight, and we assume that the lower the weight, the higher the mpg.

```{r ggplot example_negative_correlation_lm, echo=FALSE, }
df <- mtcars
model <- lm(mpg ~ wt, data = df)
df$predicted <- predict(model)
df$residuals <- residuals(model)


ggplot(df, 
       aes(x = wt,
           y = mpg)) +
  geom_point() + 
  stat_smooth(se = FALSE, 
              method = "lm",
              formula = y ~ x) +
  labs(
    title = "Cars with low weight tend to get more mpg than heavier cars",
    caption = "Source: 1974 Motor Trend US"
  )
```

The figure above shows the relationship. The points are the datapoints with x and y values for each car in the dataset, and the blue line is the regression line created by the formula $Y = f(X) + \epsilon$ . Going from a visual inspection alone, this seems to be a reasonable assumption. The points are fairly close to the regression line, and as we increase the value of $X$, the value of $Y$ decreases. This is called a negative linear relationship.

The relationship can also be positive; in that case, increasing the value of $X$ also increases the value of $Y$. Using the example from above, one might hypothesize that the amount of horsepower a car has, depends on the displacement of the car (displacement being a measure of the cylinder volume swept by the pistons).

```{r ggplot example_positive_correlation_lm, echo=FALSE, }
df <- mtcars
model <- lm(hp ~ disp, data = df)
df$predicted <- predict(model)
df$residuals <- residuals(model)


ggplot(df, 
       aes(x = disp,
           y = hp)) +
  geom_point() + 
  stat_smooth(se = FALSE, 
              method = "lm",
              formula = y ~ x) +
  labs(
    title = "Cars with higher displacement tend to have more horsepower",
    caption = "Source: 1974 Motor Trend US"
  )

```

And indeed, this seems to be the case. Compare the figure above with the previous though. Upon visual inspection, there seems to be a clear linear relationship, but as we move towards the higher value of disp, there is a notably larger distance between the regression line and the values of $Y$. This begs the question; how do we assess our models?

## 1.1 How accurate is my model?

In a traditional regression analysis the variance in $Y$ explained by $X$ is denoted $R^2$. This is scored between 0 and 1. 0, or a $R^2$ score of 0.00 means 0 % of the variance is explained by the model, and 1 (100 %) means that all the variance is explained by the model. OK - so what is a "good" value of $R^2$? Well, this is where it gets complicated. It depends on the field, the context and the variables among other things, and is out of scope for a short description that is growing longer by the minute.

Let's revisit the two previous models; mpg explained by weight, and horsepower explained by displacement.

In R, a simple way of defining a linear model is to use the lm() function, creating a model object.

```{r example_lm_1}

linear_model <- lm(mpg ~ wt, data = mtcars)
```

The syntax is lm(), and the most common arguments are *formula* and *data*. For further arguments, consult the documentation available.

The formula is given in a specific syntax, where the outcome variable (denoted $Y$) is defined on the left-hand side of the \~ (tilde) symbol, and the explanatory variable(s) on the right-hand side (denoted $X$). In our first example, the formula would be given as: mpg \~ wt. This can be read as "mpg explained by weight".

The call to lm() creates a model object, and the results can be seen using various methods. The standard one being summary().

```{r example_lm_2}

linear_model <- lm(mpg ~ wt, data = mtcars)
summary(linear_model)
```

### 1.1.1$R^2$

The summary lists various summaries of the model object that we might be interested in, such as the residuals, and the coefficients of the model. Additionally, we are given the $R^2$ score, as well as another important figure, the P-value (whether or not the relationship is statistically significant). Another option is to use the broom package. Using either tidy() or glance() gives us many of the same results, but in various other formats. When dealing with a single predictor, $R^2$ is fine, but as you add more predictors, you tend to get small increases in $R^2$, although very small unless there is actually a relationship. The Adjusted $R^2$ adjusts for the number of predictors, and should thus be used if there are more than one predictor. In our mtcars example explaining mpg by weight, we have only one, so let's look at the $R^2$. 0,75 - or 75 % of the variance in $Y$ (mpg) can be explained our model. Ok, so not bad. Let's look at our second mtcars example. Going from a visual inspection of the scatterplot, we hypothesized that this might have a weaker correlation. Let's see:\

```{r example_lm_3}

linear_model <- lm(hp ~ disp, data = mtcars)
summary(linear_model)
```

And indeed it seems that we were right! $R^2$ for the second model is 0,625, or 63 %. You can also see from the P-values of the summaries that both models are significant. While this does not imply causality, there is a very small chance that the variance explained by the model is just random chance. If our null hypothesis were that there are no relationship between mpg and weight or between horsepower and displacement, we could reject those.

### 1.1.2 Residual standard error

Another useful metric to look at is also available in the summary() output; the residual standard error or RSE. Recall the regression formula: $Y = f(X) + \epsilon$ where $\epsilon$ is the term for random errors. This is what is expressed in RSE; an estimate of the standard deviation of $\epsilon$. Roughly, this means that the RSE score is the average the output of the model will deviate from the true regression line.

Looking at our first model; the RSE is scored at 3.046. The mean value of all mpg-values is 20.09, which means that the average deviation is around 15 % off. In our second model, the RSE score is 42.65. Note that the RSE score is given in the same units as the outcome variable, in this case horsepower (hp). The mean hp-value in the mtcars dataset is 146.68, which means that the RSE is quite large; and on average the deviation will be around 29 % off. This ties in with the $R^2$ score as well; the first model explained more of the variation in the outcome variable that the second.

### 1.1.3 **Root Mean Square Error** (RMSE)

The RMSE is another similar metric for evaluating model performance. It measures the standard deviation of the residuals, which also tells us how far off the regression line the predictions are. RMSE can either be calculated by hand, or calculated by the yardstick package, where you can specify which metrics you want. Let's see an example where we get two of the three we've taken a look at:

```{r metrics_example}
df <- mtcars
model <- lm(mpg ~ wt, data = df)
df$predicted <- predict(model)
mtcars_metrics <- metric_set(rsq, rmse)
mtcars_metrics(df, truth = mpg, estimate = predicted)
```

### 1.1.4 Residuals

Residuals are another thing to consider. Defined as the difference between the predicted value of $Y$ and the true value of $Y$. In a linear regression, it is the vertical distance from the true value and the regression line. Below, this is shown with lines going from the predicted values on the regression line (shown as hollow points), and the actual values shown in color.

```{r ggplot example_residuals, echo=FALSE, }
df <- mtcars
model <- lm(mpg ~ wt, data = df)
df$predicted <- predict(model)
df$residuals <- residuals(model)



ggplot(df, aes(x = wt, y = mpg)) +
  geom_smooth(method = "lm", se = FALSE, color = "grey") +
  geom_segment(aes(xend = wt, yend = predicted), alpha = 0.4, color = "red") +
  geom_point(aes(color = residuals)) +  
  scale_color_gradient2(low = "lightblue", mid = "orange", high = "red") +  
  guides(color = FALSE) +
  geom_point(aes(y = predicted), shape = 1) + 
  labs(title = "Cars with lower weigth tend to get better mileage",
       caption = "Source: 1974 Motor Trend US") + 
  theme_bw()
```

Again, we can consider the difference between two models. Let's keep miles per gallon as our outcome variable, but look at two different predictors. IN the above model, we looked at weight. Let's consider horsepower instead.

```{r ggplot example_residuals_model2, echo=FALSE, }
df <- mtcars
model <- lm(mpg ~ hp, data = df)
df$predicted <- predict(model)
df$residuals <- residuals(model)



ggplot(df, aes(x = hp, y = mpg)) +
  geom_smooth(method = "lm", se = FALSE, color = "grey") +
  geom_segment(aes(xend = hp, yend = predicted), alpha = 0.4, color = "red") +
  geom_point(aes(color = residuals)) +  
  scale_color_gradient2(low = "lightblue", mid = "orange", high = "red") +  
  guides(color = FALSE) +
  geom_point(aes(y = predicted), shape = 1) + 
  labs(title = "Cars with lower hp tend to get better mileage",
       caption = "Source: 1974 Motor Trend US") + 
  theme_bw()
```

From a visual inspection, the residuals seem to be larger in this model, indicating a poorer fit. Let's look at the RMSE and $R^2$:

```{r compare models, echo=FALSE}
df <- mtcars
model1 <- lm(mpg ~ wt, data = df)
model2 <- lm(mpg ~ hp, data = df)
df$pred_mod1 <- predict(model1)
df$pred_mod2 <- predict(model2)
df$resid_mod1 <- residuals(model1)
df$resid_mod2 <- residuals(model2)

model_metrics <- metric_set(rsq, rmse)
mod1_res <- model_metrics(df, truth = mpg, estimate = pred_mod1)
mod2_res <- model_metrics(df, truth = mpg, estimate = pred_mod2)


p1 <- ggplot(df, aes(x = wt, y = mpg)) +
  geom_smooth(method = "lm", se = FALSE, color = "grey") +
  geom_segment(aes(xend = wt, yend = pred_mod1), alpha = 0.4, color = "red") +
  geom_point(aes(color = resid_mod1)) +  
  scale_color_gradient2(low = "lightblue", mid = "orange", high = "red") +  
  guides(color = FALSE) +
  geom_point(aes(y = pred_mod1), shape = 1) + 
  labs(title = "mpg explained by weight",
       caption = "Source: 1974 Motor Trend US") + 
  annotate("text", x = 4,5, y = 25, label = paste("R-Squared: ", round(mod1_res[1,3],2))) + 
  annotate("text", x = 4,5, y = 22, label = paste("RMSE: ", round(mod1_res[2,3],2))) +
  theme_bw()

p2 <- ggplot(df, aes(x = hp, y = mpg)) +
  geom_smooth(method = "lm", se = FALSE, color = "grey") +
  geom_segment(aes(xend = hp, yend = pred_mod2), alpha = 0.4, color = "red") +
  geom_point(aes(color = resid_mod2)) +  
  scale_color_gradient2(low = "lightblue", mid = "orange", high = "red") +  
  guides(color = FALSE) +
  geom_point(aes(y = pred_mod2), shape = 1) + 
  labs(title = "mpg explained by hp",
       caption = "Source: 1974 Motor Trend US") + 
  annotate("text", x = 250, y = 25, label = paste("R-Squared: ", round(mod2_res[1,3],2))) + 
  annotate("text", x = 250, y = 22, label = paste("RMSE: ", round(mod2_res[2,3],2))) +
  theme_bw()

grid.arrange(p1, p2, ncol = 2)
```

Graphing the models together, it is easier to see the differences. The model using hp as a predictor seems to struggle somewhat more. This can also be seen in the lower $R^2$ score, and a higher RMSE-score.

## 1.2 Splitting data

Until now though, we have modeled and predicted on the same dataset. This is risky, and not a good estimate on how our model would perform if we added new cars to the dataset. The norm is instead to split the data into several datasets. One strategy is to use a train/test split, where the larger part (often 70 or 80 %) of the data is spent on training the model, and then the remaining is used as a test set, where we test to see what kind of accuracy the model will have on unseen data. A second option is to create three sets, the training data (70 %), a validation set (15 %) and a test set (15%). This is typically done if we want to check multiple different models or algorithms prior to selecting one to expose on the test data.

We will be using the rsample package found in tidymodels to perform the split, and we'll go with a regular train/test split. For this, we are going to use a larger dataset, the built-in dataset called diamonds, which contains data about approximately 50.000 diamonds. We are going to use this to train a linear regression model, and use all available features to see if we can predict the sales price. This is a multiple regression, since we now have more than one predictor variable.

```{r create datasets}

set.seed(42) # set the seed so that future iterations will have the same results
diamond_split <- initial_split(diamonds, prop = 0.8)
train <- diamond_split %>%
  training()

test <- diamond_split %>% 
  testing()

mean(train$price)
mean(test$price)
mean(diamonds$price)

```

Sometimes, splitting datasets can create class imbalance. To check for this, I've compared the mean sales price of the test, training and original datasets. If imbalance seemed to be a problem, we could've used the stata argument in initial_split(), specifying a stratification on the price variable.

## 1.3 Training the model

Later on, we will start using the parsnip package to train models, but for now, let's keep going with the lm() function.

```{r create datasets2, echo=FALSE }

set.seed(42) # set the seed so that future iterations will have the same results
diamond_split <- initial_split(diamonds, prop = 0.8)
train <- diamond_split %>%
  training()

linear_model <- lm(price ~ ., data = train)
metrics <- metric_set(rsq, rmse)
train$predicted <- predict(linear_model)
metrics <- metric_set(rsq, rmse)
metrics(train, truth = price, estimate = predicted)

```

Our model shows an $R^2$ of 92 %, and RMSE is 1128 - meaning that on average, we will be off by 1128 dollars. Given that the mean price is around 3900, this is somewhat high. Let's see what we can do with this. Let's start by visualizing the data we are interested in - the price. Linear regression works better on data that is normally distributed, so let's start with a histogram.

```{r histogram of training data, echo=FALSE}
set.seed(42) # set the seed so that future iterations will have the same results
diamond_split <- initial_split(diamonds, prop = 0.8)
train <- diamond_split %>%
  training()

ggplot(train, aes(x = price)) + 
  geom_histogram()
```

Hm! A large number of the diamonds are between 0 and 5000 dollar, meaning that we have a right-skewed distribution. Let's see if plotting this on a log_10 scale helps:

```{r histogram of training data with log_10, echo=FALSE}
set.seed(42) # set the seed so that future iterations will have the same results
diamond_split <- initial_split(diamonds, prop = 0.8)
train <- diamond_split %>%
  training()

ggplot(train, aes(x = price)) + 
  geom_histogram() + 
  scale_x_log10()
```

Ok - this looks better. It's not completely normally distributed, but it certainly looks better. Let's perform a log transformation of our data:

```{r training the model and assessing fit, echo=FALSE}
set.seed(42) # set the seed so that future iterations will have the same results
diamond_split <- initial_split(diamonds, prop = 0.8)
train <- diamond_split %>%
  training()

train$price <- log10(train$price)

linear_model <- lm(price ~ ., data = train)
metrics <- metric_set(rsq, rmse)
train$predicted <- predict(linear_model)
metrics <- metric_set(rsq, rmse)
metrics(train, truth = price, estimate = predicted)
```

After the transformation, we get a RMSE-score of 0.07. Now, did we go from predicting 1128 off the true value on average to 7 cents on average? No - we didn't. The outcome variable has been log10 transformed, so the units are no longer in dollars. What this score means is that on average, our predictions will be 7 % off the geometric mean. Consider that with a mean of around \$3900 and an error of \$1128, we were off by around 29 % before the log transformation - so this is a rather good improvement. Now, let's see how well our model performs on unseen data; the test set.

## 1.4 Testing the model

```{r histogram of training data, log transformed, echo=FALSE}
set.seed(42) # set the seed so that future iterations will have the same results
diamond_split <- initial_split(diamonds, prop = 0.8)
train <- diamond_split %>%
  training()
test <- diamond_split %>%
  testing()

train$price <- log10(train$price)
test$price <- log10(test$price)
linear_model <- lm(price ~ ., data = train)
test$predicted <- predict(linear_model, newdata = test)

metrics <- metric_set(rsq, rmse)
metrics(test, truth = price, estimate = predicted)
```

And there we have it. The final $R^2$ score is a bit lower than on the training data, but not by a lot. RMSE is very similar, although a bit higher than on the test set, which is what we would expect given that the $R^2$ was lower as well.

## 1.5 A note on the workflow

In the later
